services:

  kafka-broker:
    image: bitnami/kafka:latest
    container_name: kafka-broker
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka-broker:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://kafka-broker:9092,CONTROLLER://kafka-broker:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka-broker:9092
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_KRAFT_CLUSTER_ID_FILE=/bitnami/kafka/storage/meta.properties
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LOG_RETENTION_HOURS=72
    volumes:
      - kafka-data:/bitnami/kafka
    healthcheck:
      test: [ "CMD-SHELL", "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka-broker:9092 --list || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  postgres:
    image: postgres:latest
    container_name: postgres
    restart: no
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: eonet
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./kafka-postgres/postgres-init.sql:/docker-entrypoint-initdb.d/postgres-init.sql

  kafka-topics-init:
    image: bitnami/kafka:latest
    container_name: kafka-topics-init
    depends_on:
      kafka-broker:
        condition: service_healthy
    volumes:
      - ./scripts:/scripts
    entrypoint: ["/bin/sh", "/scripts/init-kafka-topics.sh"]

  poller:
    build:
      context: ./poller
    container_name: eonet-poller
    depends_on:
      - kafka-broker
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-broker:9092
      - POLL_MODE=dev # remember to set this to `prod` to ingest entire `events.json`

  flink-jobmanager:
    image: flink:2.0.0
    container_name: flink-jobmanager
    ports:
      - "8081:8081"  # Flink UI
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    depends_on:
      - kafka-broker
      - kafka-topics-init
    volumes:
      - ./flink-sql:/opt/sql
      - ./flink-plugins/flink-sql-connector-kafka-4.0.0-2.0.jar:/opt/flink/lib/flink-sql-connector-kafka-4.0.0-2.0.jar

  flink-taskmanager:
    image: flink:2.0.0
    container_name: flink-taskmanager
    depends_on:
      - flink-jobmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager # how many expected streaming jobs running
      - | 
        FLINK_PROPERTIES=
        taskmanager.numberOfTaskSlots: 4
    volumes:
      - ./flink-sql:/opt/sql
      - ./flink-plugins/flink-sql-connector-kafka-4.0.0-2.0.jar:/opt/flink/lib/flink-sql-connector-kafka-4.0.0-2.0.jar

  kafka-connect:
    image: confluentinc/cp-kafka-connect:latest
    container_name: kafka-connect
    depends_on:
      - kafka-broker
      - postgres
    ports:
      - "8083:8083"  # Kafka Connect REST API
    environment:
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_BOOTSTRAP_SERVERS: kafka-broker:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    volumes:
      - ./kafka-postgres/connect-plugins:/usr/share/confluent-hub-components
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 3s

  kafka-connect-init:
    image: curlimages/curl:latest
    depends_on:
      kafka-connect:
        condition: service_healthy
    entrypoint: >
      sh -c "
      /scripts/wait-for-kafka-connect.sh &&
      echo 'Deleting old Kafka JDBC Sink Connector...' &&
      curl -X DELETE http://kafka-connect:8083/connectors/postgres-sink-connector || true &&
      echo 'Registering new Kafka JDBC Sink Connector...' &&
      curl -X POST http://kafka-connect:8083/connectors -H 'Content-Type: application/json' --data '@/init/connect-jdbc-sink.json'
      "
    volumes:
      - ./kafka-postgres/kafka-init:/init
      - ./kafka-postgres/scripts:/scripts

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka-broker
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: "local"
      KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka-broker:9092

volumes:
  kafka-data:
  postgres-data:

